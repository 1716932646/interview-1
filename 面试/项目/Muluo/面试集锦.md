### 介绍下你的项目

仿照 Muduo 实现的基于 Reactor 模式的 Linux C++多线程网络库, 集成 MuLog 异步日志，同时内嵌一个简洁HTTP 服务器，实现路由分发及静态资源访问。

## Feature

* 底层使用Epoll LT模式实现I/O复用，非阻塞I/O
* 多线程、定时器依赖于c++11提供的std::thread、std::chrono库
* Reactor模式，主线程accept请求后，使用Round Robin分发给线程池中线程处理
* 基于小根堆的定时器管理队列
* 双缓冲技术实现的异步日志
* 使用智能指针及RAII机制管理对象生命期
* 使用状态机解析HTTP请求，支持HTTP长连接
* HTTP服务器支持URL路由分发及访问静态资源，可实现RESTful架构

### 网络库的io模型是怎么样的，为什么这个io模型是高性能的？

UNP中总结的IO模型有5种之多：`阻塞IO，非阻塞IO，IO复用，信号驱动IO，异步IO`。前四种都属于同步IO。

*阻塞IO不必说了。

* 非阻塞IO ，IO请求时加上O_NONBLOCK一类的标志位，立刻返回，IO没有就绪会返回错误，需要请求进程主动轮询不断发IO请求直到返回正确。
* IO复用同非阻塞IO本质一样，不过利用了新的select等系统调用，由内核来负责本来是请求进程该做的轮询操作。看似比非阻塞IO还多了一个系统调用开销，不过因为可以支持多路IO，才算提高了效率。

* 信号驱动IO，调用sigaltion系统调用，当内核中IO数据就绪时以SIGIO信号通知请求进程，请求进程再把数据从内核读入到用户空间，这一步是阻塞的。 
* 异步IO，如定义所说，不会因为IO操作阻塞，IO操作全部完成才通知请求进程。

`Reactor[one loop per thread: non-blocking + IO multiplexing]`模型。muduo采用的是Reactors in thread有一个main Reactor负责accept(2)连接，然后把连接挂在某个sub Reactor中(muduo中采用的是round-robin的方式来选择sub Reactor)，这样该连接的所有操作都在那个sub Reactor所处的线程中完成。多个连接可能被分到多个线程中，以充分利用CPU。

muduo采用的是固定大小的Reactor pool，**池子的大小通常根据CPU数目确定**。也就是说线程数固定，这样程序的总体处理能力不会随连接数增加而下降。另外一个连接完全由一个线程管理，那么请求的顺序性有保证，突发请求也不会占满8个核(如果需要优化突发请求，可以考虑Reactors + thread pool)。这种方案把IO分派给多个线程，防止出现一个Reactor的处理能力饱和。



### muduo的多线程体现在什么地方？

> muduo是基于one loop per thread模型。字面意思上讲就是每个线程里有个loop，即消息循环。服务器必定有一个监听的socket和1到N个连接的socket，每个socket也必定有网络事件。我们可以启动设定数量的线程，让这些线程来承担网络事件。

> 每个进程默认都会启动一个线程，即这个线程不需要我们手动创建，称之为主线程。一般地我们让主线程来承担监听socket的网络事件，然后等待新的连接。至于新连接的socket的事件要不要在主线程中处理，这个得看我们启动其他线程即工作线程的数量。如果启动了工作线程，**那么新连接的socket的网络事件一定是在工作线程中处理的。**

> 每个线程的事件处理都是在一个EventLoop的while循环中，而每个EventLoop都有一个多路事件复用解析器epoller。循环的主体部分是等待epoll事件触发，从而处理事件。主线程EventLoop的epoller会监听socket可读事件，而工作线程一开始什么都没有添加，因为还没有连接产生。在没有事件触发之前，epoller都是阻塞的。导致线程被挂起。

> 当有连接到来时，挂起的主线程恢复，会执行新连接的回调函数。在该函数中，会从线程池中取得一个线程来接管新的socket的处理。

> muduo中多线程主要是`EventLoopThread(IO线程类)`、`EventLoopThreadPool(IO线程池类)`，IO线程池的功能是开启若干个IO线程，并让这些IO线程处于事件循环的状态。

> `muduo` 采用`one loop per thread`的设计思想，即每个线程运行一个循环，这里的循环就是事件驱动循环`EventLoop`。所以，`EventLoop`对象的loop函数，包括间接引发的`Poller`的`poll`函数，`Channel`的`handleEvent`函数，以及`TcpConnection`的`handle*`函数都是在一个线程内完成的。而在整个`muduo`体系中，有着多个这样的`EventLoop`，每个`EventLoop`都执行着`loop,poll,handleEvent,handle*`这些函数。这种设计模型也被称为`Reactor + 线程池`。

> 控制这些`EventLoop`的，保存着事件驱动循环线程池的，就是TcpServer类。顾名思义，服务器类，用来创建一个高并发的服务器，内部便有一个线程池，线程池中有大量的线程，每个线程运行着一个事件驱动循环，即`one loop per thread`。

> 另外，`TcpServer`本身也是一个线程(主线程)，也运行着一个`EventLoop`， 这个事件驱动循环仅仅用来监控客户端的连接请求，即`Accetpor`对象的`Channel`的可读事件。 通常，如果用户添加定时器任务，也会由这个EventLoop监听。但是`TcpServer`的这个`EventLoop`不在线程池中，这一点要区分开，线程池中的线程只用来运行负责监控`TcpConnection`的`EventLoop`的。



### 你用了epoll，说一下为什么用epoll，还有其他复用方式吗？区别是什么？

——先说说其他的复用方式吧，比较常用的有三种：select/poll/epoll。本项目之所以采用epoll，[参考问题（Why is epoll faster than select?）](https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/17355593/why-is-epoll-faster-than-select)



* 对于select和poll来说，所有文件描述符都是在用户态被加入其文件描述符集合的，**每次调用都需要将整个集合拷贝到内核态**；epoll则将整个文件描述符集合维护在内核态，每次添加文件描述符的时候都需要**执行一个系统调用**。系统调用的开销是很大的，而且在有很多短期活跃连接的情况下，epoll可能会慢于select和poll由于这些大量的系统调用开销。
* select使用线性表描述文件描述符集合，**文件描述符有上限**；poll使用**链表来描述**；epoll底层通过红黑树来描述，并且维护一个ready list，将事件表中已经就绪的事件添加到这里，在使用epoll_wait调用时，仅观察这个list中有没有数据即可。
* select和poll的最大开销来自内核判断是否有文件描述符就绪这一过程：每次执行select或poll调用时，**它们会采用遍历的方式**，遍历整个文件描述符集合去判断各个文件描述符是否有活动；epoll则不需要去以这种方式检查，当有活动产生时，**会自动触发epoll回调函数通知epoll文件描述符**，然后内核将这些就绪的文件描述符放到之前提到的**ready list中等待epoll_wait调用后被处理**。
* select和poll都只能工作在**相对低效的LT模式**下，而epoll同时支持LT和ET模式。
* 综上，**当监测的fd数量较小**，且各个fd都很活跃的情况下，建议使用select和poll；**当监听的fd数量较多**，且单位时间仅部分fd活跃的情况下，使用epoll会明显提升性能。



### [epoll的边沿触发和水平触发有什么区别？(epoll的源码并不长，从源码的角度回答比较好)](https://www.cnblogs.com/charlesblc/p/6242479.html)

 **水平触发(level-trggered)**

- 只要文件描述符关联的读内核缓冲区非空，有数据可以读取，就一直发出可读信号进行通知，
- 当文件描述符关联的内核写缓冲区不满，有空间可以写入，就一直发出可写信号进行通知

**LT模式支持阻塞和非阻塞两种方式。epoll默认的模式是LT。**

 **边缘触发(edge-triggered)**

- 当文件描述符关联的读内核缓冲区由空转化为非空的时候，则发出可读信号进行通知，
- 当文件描述符关联的内核写缓冲区由满转化为不满的时候，则发出可写信号进行通知
- 两者的区别在哪里呢？水平触发是只要读缓冲区有数据，就会一直触发可读信号，而边缘触发仅仅在空变为非空的时候通知一次。
- 执行epoll_create时，创建了**红黑树和就绪链表**，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后**向内核注册回调函数**，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。

[![img](https://camo.githubusercontent.com/8aaaa5b6aadae73efe7cb7159729e773cf323f4a63e411b4588d1c0e348c1fd4/68747470733a2f2f696d61676573323031352e636e626c6f67732e636f6d2f626c6f672f3839393638352f3230313730312f3839393638352d32303137303130323134343731343232322d3331383836363734392e706e67)](https://camo.githubusercontent.com/8aaaa5b6aadae73efe7cb7159729e773cf323f4a63e411b4588d1c0e348c1fd4/68747470733a2f2f696d61676573323031352e636e626c6f67732e636f6d2f626c6f672f3839393638352f3230313730312f3839393638352d32303137303130323134343731343232322d3331383836363734392e706e67)

- LT, ET这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，**会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表**，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是**LT模式的句柄**了），**并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了**。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回这个句柄。（从上面这段，可以看出，LT还有个**回放的过程，低效了**）







### c++11多线程的实现

`std::thread th1(proc1);`创建了一个名为th1的线程，并且线程th1开始执行。实例化`std::thread`类对象时，至少需要传递函数名作为参数。如果函数为有参函数,如`void proc2(int a,int b)`则`std::thread th2(proc2,a,b);`



**当线程启动后，一定要在和线程相关联的std::thread对象销毁前，对线程运用join()或者detach()方法**



**互斥锁**

互斥量mutex就是互斥锁，加锁的资源支持互斥访问；

**读写锁**

`shared_mutex`读写锁把对共享资源的访问者划分成读者和写者，多个读线程能同时读取共享资源，但只有一个写线程能同时读取共享资源

`shared_mutex`

* 通过`lock_shared`，`unlock_shared`进行读者的锁定与解锁；

* 通过`lock`，`unlock`进行写者的锁定与解锁。



#### **lock_guard:**

其原理是：声明一个局部的`std::lock_guard`对象，在其构造函数中进行加锁，在其析构函数中进行解锁。最终的结果就是：创建即加锁，作用域结束自动解锁。从而使用`std::lock_guard()`就可以替代`lock()`与`unlock()`。注意mutex或者shared_mutex可少不了





### **unique_lock:**

std::unique_lock类似于lock_guard,只是std::unique_lock用法更加丰富。 使用std::lock_guard后不能手动lock()与手动unlock();使用std::unique_lock后可以手动lock()与手动unlock(); 



std::lock_gurad也可以传入两个参数，第一个参数为adopt_lock标识时，表示构造函数中不再进行互斥量锁定，因此**此时需要提前手动锁定**。

std::unique_lock的第二个参数，除了可以是adopt_lock,还可以是try_to_lock与defer_lock;

try_to_lock: 尝试去锁定，**得保证锁处于unlock的状态**,然后尝试现在能不能获得锁；尝试用的lock()去锁定这个mutex，但如果没有锁定成功，会立即返回，不会阻塞在那里，并继续往下执行；

defer_lock: 始化了一个没有加锁的mutex;



### **condition_variable:**

std::condition_variable类搭配std::mutex类来使用，condition_variable对象的作用不是用来管理互斥量的，它的作用是用来同步线程，



`wait(locker) :`wait函数需要传入一个std::mutex（一般会传入std::unique_lock对象）,即上述的locker。wait函数会自动调用 locker.unlock() 释放锁（因为需要释放锁，所以要传入mutex）并阻塞当前线程，本线程释放锁使得其他的线程得以继续竞争锁。



一旦当前线程获得notify(通常是另外某个线程调用` notify_* `唤醒了当前线程)，`wait()` 函数此时再自动调用 `locker.lock()`上锁。



cond.notify_one(): 随机唤醒一个等待的线程

cond.notify_all(): 唤醒所有等待的线程



**2.3 异步线程**(不熟悉)

需要`#include<future>`

### **async与future：**

std::async是一个函数模板，用来启动一个异步任务，它返回一个std::future类模板对象，future对象起到了**占位**的作用. 占位是什么意思？就是说该变量现在无值，但将来会有值

刚实例化的future是没有储存值的，但在调用std::future对象的get()成员函数时，主线程会被阻塞直到异步线程执行结束，并把返回结果传递给std::future，即通过FutureObject.get()获取函数返回值。

```C++
#include <iostream>
#include <thread>
#include <mutex>
#include<future>
#include<Windows.h>
using namespace std;
double t1(const double a, const double b)
{
 double c = a + b;
 Sleep(3000);//假设t1函数是个复杂的计算过程，需要消耗3秒
 return c;
}

int main() 
{
 double a = 2.3;
 double b = 6.7;
 future<double> fu = async(t1, a, b);//创建异步线程线程，并将线程的执行结果用fu占位；
 cout << "正在进行计算" << endl;
 cout << "计算结果马上就准备好，请您耐心等待" << endl;
 cout << "计算结果：" << fu.get() << endl;//阻塞主线程，直至异步线程return
        //cout << "计算结果：" << fu.get() << endl;//取消该语句注释后运行会报错，因为future对象的get()方法只能调用一次。
 return 0;
}
```



### **shared_future**

std::future与std::shard_future的用途都是为了**占位**，但是两者有些许差别。std::future的get()成员函数是转移数据所有权;std::shared_future的get()成员函数是复制数据。 因此： **future对象的get()只能调用一次**；无法实现多个线程等待同一个异步线程，一旦其中一个线程获取了异步线程的返回值，其他线程就无法再次获取。 **std::shared_future对象的get()可以调用多次**；可以实现多个线程等待同一个异步线程，每个线程都可以获取异步线程的返回值。



### **原子类型atomic<>**

原子操作指“不可分割的操作”.互斥量的加锁一般是针对一个代码段，而原子操作针对的一般都是一个变量(操作变量时加锁防止他人干扰)。 std::atomic<>是一个模板类，使用该模板类实例化的对象，提供了一些保证原子性的成员函数来实现共享数据的常用操作。

std::atomic<>对象提供了常见的原子操作（通过调用成员函数实现对数据的原子操作）： store是原子写操作，load是原子读操作。exchange是于两个数值进行交换的原子操作。 **即使使用了std::atomic<>，也要注意执行的操作是否支持原子性**





### **线程池**

**不采用线程池时：**

创建线程 -> 由该线程执行任务 -> 任务执行完毕后销毁线程。即使需要使用到大量线程，每个线程都要按照这个流程来创建、执行与销毁。

虽然创建与销毁线程消耗的时间 远小于 线程执行的时间，但是对于需要频繁创建大量线程的任务，创建与销毁线程 所占用的**时间与CPU资源**也会有很大占比。

**为了减少创建与销毁线程所带来的时间消耗与资源消耗，因此采用线程池的策略：**

程序启动后，预先创建一定数量的线程放入空闲队列中，这些线程都是处于阻塞状态，基本不消耗CPU，只占用较小的内存空间。

接收到任务后，任务被挂在任务队列，线程池选择一个空闲线程来执行此任务。

任务执行完毕后，不销毁线程，线程继续保持在池中等待下一次的任务。

**线程池所解决的问题：**

(1) 需要频繁创建与销毁大量线程的情况下，由于线程预先就创建好了，接到任务就能马上从线程池中调用线程来处理任务，**减少了创建与销毁线程带来的时间开销和CPU资源占用**。

(2) 需要并发的任务很多时候，无法为每个任务指定一个线程（线程不够分），使用线程池可以将提交的任务挂在任务队列上，等到池中有空闲线程时就可以为该任务指定线程。



### 定时器相关

* 为什么要用定时器？
  ——处理定时任务，或者非活跃连接，节省系统资源；
* 说一下定时器的工作原理

数据结构的选择包括:1. 双向有序链表，2. 堆， 3. 时间轮， 4.层级时间轮。

——服务器就为各事件分配一个定时器。该项目使用SIGALRM信号来实现定时器，首先每一个定时事件都处于一个升序链表上，**通过alarm()函数周期性触发SIGALRM信号**，而后信号回调函数利用管道通知主循环，主循环接收到信号之后对升序链表上的定时器进行处理：若一定时间内无数据交换则关闭连接。


* 双向链表啊，删除和添加的时间复杂度说一下？还可以优化吗？
  ——添加一般情况下都是O(N)，删除只需要O(1)。从双向链表的方式优化不太现实，可以考虑使用最小堆、

* 最小堆优化？说一下时间复杂度和工作原理
  ——最小堆以每个定时器的过期时间进行排序，最小的定时器位于堆顶，当SIGALRM信号触发tick（）函数时执行过期定时器清除，如果堆顶的定时器时间过期，则删除，并重新建堆，再判定是否过期，如此循环直到未过期为止。

  本项目手写数组模拟的二叉堆以及使用STL priority_queue实现











### reactor、proactor、主从reactor模型的区别？

* Reactor模式：要求主线程（I/O处理单元）只负责监听文件描述符上是否有事件发生（可读、可写），若有，则立即通知工作线程，将socket可读可写事件放入请求队列，**读写数据、接受新连接及处理客户请求均在工作线程中完成。(需要区别读和写事件)**
* Proactor模式：主线程和内核负责处理读写数据、接受新连接等**I/O操作，工作线程仅负责业务逻辑（给予相应的返回url）**，如处理客户请求。
* 主从Reactor模式：核心思想是，主反应堆线程只负责分发Acceptor连接建立，已连接套接字上的I/O事件交给sub-reactor负责分发。其中 sub-reactor的数量，可以根据CPU的核数来灵活设置。

下图即是其工作流程：
![image](https://pic2.zhimg.com/v2-9e07d2da99253d5330fac1c953521791_b.jpg)
**主反应堆线程一直在感知连接建立的事件**，如果有连接成功建立，主反应堆线程通过accept方法获取已连接套接字，**接下来会按照一定的算法选取一个从反应堆线程**，并把已连接套接字**加入到选择好的从反应堆线程中**。主反应堆线程唯一的工作，就是调用accept获取已连接套接字，以及将已连接套接字加入到从反应堆线程中。







## HTTP

**解析**
对于http请求的解析，使用HttpParser，主要是使用有限状态机 + 正则来进行字符串匹配。

**路由**
这里的设计参考了golang的mux包，首先Router下有多个Route，每个Route下又有多个Matcher。

每个Matcher都对应了一条匹配条件，比如要求请求的方法为GET。当一个请求匹配了Route下的所有条件，即匹配成功这个Route，就可以得到该Route的处理函数来处理Request了。如果没有匹配的Route，那么就使用默认的处理函数，即返回404页面。

Matcher的匹配主要是通过正则表达式来进行，用户可以利用正则表达式来取得一些参数。


用户可以根据需要设置路由及匹配条件，目前支持URL、请求参数、请求头及请求方法的匹配，并且可以从中提取出参数。
一个请求必须满足所有条件才能匹配这个路由，得到它的Handler。

首先要设置相应的路由处理函数,该函数由用户实现，如果要实现静态资源访问，则需使用HttpServer的文件处理函数,该函数会将访问映射到用户指定的路径。

#### 静态资源

文件的访问由[File](https://github.com/silence1772/Sinetlib/blob/master/src/http/file.h)处理，它主要是提供该文件或目录的相关信息，并通过mmap内存映射方式将文件读取出来。

当用户访问资源时，相应的处理函数由[FileHandler](https://github.com/silence1772/Sinetlib/blob/master/src/http/filehandler.h)提供。



